# 

# 笔记

========================================

应该一开始就记笔记的，但是又不知道从何记起，于是一直搁置了，忙了一些无意义的事情再看之前学的东西，有些尴尬，基本都记得很模糊了，于是还是写一点东西吧，从现在开始写，前面的东西会再补的。

接下来是笔记

## 文本任务在深度学习中的处理

### Word2vector

**一个很好的博客：[如何通俗理解Word2Vec (23年修订版)-CSDN博客](https://blog.csdn.net/v_JULY_v/article/details/102708459)**

首先说明one-hot编码，个人认为是一种信息密度较低的编码方式，但是很直观简单。

#### one-hot编码

one-hot编码的向量，只有一个元素为1，其余都为0，例如:[0,1,0,0,0,0,0]<sup>T</sup>

用one-hot进行文本表示，是在一个大小为N的词库V中，用一个N维向量去表示一个词，是第几个词就在向量中第几个元素为1。

如果我们使用one-hot向量作为输入向量，使用**NNLM模型**去预测前n-1个词是w<sub>1</sub>-w<sub>n-1</sub>时第n个词是w<sub>n</sub>的概率——将前n-1个词的one-hot向量进行收尾相接作为输入向量， 经过隐藏层到输出层，softmax之后求出是每个词的概率。这样的模型很耗时，因为词库很大，维度可能高达几百万，one-hot向量的编码维度就很大。

#### 词嵌入(Word Embeddings)-Word2Vec

word2vec有两种模型，分别是CBOW和SkipGram：

- **CBOW(Continuous Bag-of-Word)：** 以上下文词汇去预测当前词

- **SkipGram：** 以当前词汇去预测上下文    

这里主要说说CBOW（两种模型计算方法差不多）

所需要的是一个三层的神经网络（下图为一示例）：

![三层神经网络](.\pic\word2vec(1).png)

三层分别为：输入层、**投影层**、输出层——输入层到投影层需要乘上一个权重矩阵W，**没有激活函数和偏置b**；而投影层到输出层乘上一个权重矩阵W '，**有激活函数和偏置b**，最后需要softmax。对于一个训练样本(w<sub>o</sub>,w<sub>i</sub>)，输入为w<sub>o</sub>，真实值为w<sub>i</sub>。损失函数推导如下：

![loss1](.\pic\loss(1).png)

习惯最小化损失函数，因此定义损失函数为：

![loss2](.\pic\loss(2).png)

明显的是，输入的向量为one-hot向量，所以投影层向量就是取出W中的对应列(或行，跟表示方法有关系)——(这个特性也让输入层到投影层的计算相对很简单)。这个就是词向量，词向量维度远小于词库大小， 起到很好的降维作用。

有明显的问题是，投影层到输出层以及softmax等的计算很耗时，需要优化——

#### 优化策略(针对于隐藏层到输出层)：

#### -层次Softmax(Hierarchical SoftMax) 和 负采样(Negative Sampling)

**层次Softmax**，对于CBOW的优化，基于霍夫曼树，每一个节点（非叶子节点）有一个参数向量，每一个叶子节点代表一个词（这样到每个词的路径是唯一且确定的），值得注意的是，它的结构在训练之前应该确定好，训练时层数不发生变化。模型在做词预测时，对于常见词，只需要较少的步骤就可以达到预测结果，而对于不常见的词，虽然需要更多的步骤，但由于出现频率较低，对总体计算量的影响较小。

在层次Softmax中，我们不再计算整个词汇表的概率，而是通过路径的概率来进行预测（沿着路径把概率权重乘起来）。每个路径上的节点都代表一次二分类决策，目标是最大化路径上每个节点的概率。优化上相当于给复杂度开一个log。核心思想是多分类->二分类。

**负采样**，对于SkipGram的优化：

不计算所有词的概率，而是让模型去区分好和坏（正样本和负样本）

- **正样本**：从实际文本中出现的（中心词，上下文词）对。例如，对于句子 “The quick brown fox jumps”，如果中心词是 “quick”，那么正样本就是 (“quick”, “The”), (“quick”, “brown”) 等。

- **负样本**：我们人为地“伪造”一些词对。我们保持中心词不变，但从词汇表中 **随机抽取** `k` 个词（比如 5 个或 10 个）作为“上下文词”。这些随机词大概率不会出现在中心词的真实上下文中。例如，(“quick”, “pizza”), (“quick”, “computer”) 等就是负样本。

**新的目标函数：**  
对于每一个训练样本（一个中心词和一个上下文词），我们：

1. 最大化正样本对的概率（即，模型应判断它为“1”或“真”）。

2. 最小化 k 个负样本对的概率（即，模型应判断它们为“0”或“假”）。

负采样并不是完全均匀地随机采样。它的采样分布是一个 **加权分布**：

- **高频词** 被选为负样本的概率更大。

- **低频词** 被选为负样本的概率更小。

具体来说，采样概率 P(wi​) 与词频 f(wi​) 的 3/4 次方成正比。
