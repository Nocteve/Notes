# LSTM

### 记录一些LSTM的笔记

#### 首先从RNN讲起

**RNN（循环神经网络）是一类专为序列数据设计的神经网络，让神经网络具有记忆功能**

一个核心基本概念是隐状态h（hidden state），**隐状态h可以对序列形的数据提取特征，接着再转换为输出**，是RNN的记忆功能所在。

另一个核心基本的概念是**时间步**，可以理解成一个序列中连续的各个单元的位置，比如一句话中每个词就是一个时间步，一个视频中各个帧就是一个时间步。从这个概念中我们可以注意到，RNN并不是一次性吞下所有数据（比如全连接神经网络），而是一步一步的处理序列数据的。

时间步也是RNN(**循环**神经网络)的一个循环单元，所以我们阐明如何处理每个时间步的，在每一个时间步，RNN会做以下事情：

- 接收输入：接受该时间步的输入向量x<sub>t</sub>；

- 结合记忆：结合上一个时间步的隐状态h<sub>t-1</sub>（RNN的“记忆”）

- 计算更新：h<sub>t</sub> = f(W * x<sub>t</sub>+ U * h<sub>t-1</sub>+b)——值得注意的是，对于每一步时间步，参数W,U,b都是一样的，每一个时间步共享这些参数，也就是RNN对于这些参数循环；这里的f是激活函数(如ReLU,tanh,Sigmoid)，这里顺便说一下，对于激活函数tanh和Sigmoid,它们其实是差不多的，主要的区别在于，tanh把函数映射到(-1,1)的区间中，而Sigmoid把函数映射到(0,1)区间。

- 输出：对于每个时间步的隐藏状态h，通过一层全连接层，y<sub>t</sub> = g(V * h<sub>t</sub> + c)，得到输出，这里的参数V和c也是各个时间步共享的，y<sub>1</sub>、y<sub>2</sub>、......y<sub>n</sub>构成和输入序列长度相等的输出序列。

综上，RNN的处理过程是对于同一向量(eg.一个句子)的不同时刻(eg.各个词语)




















































































































































