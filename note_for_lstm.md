# LSTM

### 记录一些LSTM的笔记

#### 首先从RNN讲起

**RNN（循环神经网络）是一类专为序列数据设计的神经网络，让神经网络具有记忆功能**

一个核心基本概念是隐状态h（hidden state），**隐状态h可以对序列形的数据提取特征，接着再转换为输出**，是RNN的记忆功能所在。

另一个核心基本的概念是**时间步**，可以理解成一个序列中连续的各个单元的位置，比如一句话中每个词就是一个时间步，一个视频中各个帧就是一个时间步。从这个概念中我们可以注意到，RNN并不是一次性吞下所有数据（比如全连接神经网络），而是一步一步的处理序列数据的。

时间步也是RNN(**循环**神经网络)的一个循环单元，所以我们阐明如何处理每个时间步的，在每一个时间步，RNN会做以下事情：

- 接收输入：接受该时间步的输入向量x<sub>t</sub>；

- 结合记忆：结合上一个时间步的隐状态h<sub>t-1</sub>（RNN的“记忆”）

- 计算更新：h<sub>t</sub> = f(W * x<sub>t</sub>+ U * h<sub>t-1</sub>+b)——值得注意的是，对于每一步时间步，参数W,U,b都是一样的，每一个时间步共享这些参数，也就是RNN对于这些参数循环；这里的f是激活函数(如ReLU,tanh,Sigmoid)，这里顺便说一下，对于激活函数tanh和Sigmoid,它们其实是差不多的，主要的区别在于，tanh把函数映射到(-1,1)的区间中，而Sigmoid把函数映射到(0,1)区间。

- 输出：对于每个时间步的隐藏状态h，通过一层全连接层，y<sub>t</sub> = g(V * h<sub>t</sub> + c)，得到输出，这里的参数V和c也是各个时间步共享的，y<sub>1</sub>、y<sub>2</sub>、......y<sub>n</sub>构成和输入序列长度相等的输出序列。

![RNN_pic](./RNN_pic.png)

综上，RNN的处理过程是对于同一向量(eg.一个句子)的不同时刻(eg.各个词语)

RNN的问题是它只具有短时记忆，比如使用tanh激活函数，它的导数值是小于1的，那么如果用较后时间步的输出（或隐藏状态）去反向传播更新较前时间步的参数，根据链式法则，求导会有很多小于1的数乘在一起，结果会十分接近于0，对于更新参数会几乎不起作用，这个问题是梯度消失；当然， 随着计算越来越复杂，也有可能导致另一个问题梯度爆炸。

接下来是RNN的一个变体，**LSTM**，可以在一定程度上解决梯度消失和梯度爆炸这两个问题。

#### LSTM（Long ShortTerm）

![LSTM_pic](/home/nocteve/code/Notes/LSTM_pic.png)

σ表示的Sigmoid激活函数，与tanh函数类似，不同之处在于sigmoid是把值压缩到0~1之间而不是-1~1之间。这样的设置有助于更新或忘记信息，极端地看，乘0就会被完全忘掉，乘1就会被完全记住。

LSTM的核心思想是细胞状态，水平线在图上方贯穿运行。

![LSTM_cell](/home/nocteve/code/Notes/LSTM_cell.png)

这条贯穿的细胞状态链上蕴含着这个序列到目前时间步的信息，对它我们只有少量的线性交互。

LSTM通过“门”这种巧妙的结构去增删细胞状态的信息，一共有三种类型的门结构，分别是：遗忘门、输入门、输出门。

- **遗忘门**

LSTM的第一步是通过遗忘门决定从细胞状态中遗忘什么信息，遗忘门读取上一个输出h<sub>t-1</sub>和当前输入x<sub>t</sub>，做一个Sigmoid变换，即f<sub>t</sub> = σ(W<sub>f</sub> * [h<sub>t-1</sub> , x<sub>t</sub>] + b<sub>f</sub>) , f<sub>t</sub>所有元素在0～1之间，它直接与细胞状态C<sub>t-1</sub>相乘，决定以何种程度遗忘哪些信息。

- **输入门**

包含两个层，一个tanh层，一个Sigmoid层，tanh层产生一个候选信息向量C<sub>t</sub><sup>~</sup>，Sigmoid层产生一组权重i<sub>t</sub>去筛选掉不重要的信息（和遗忘门的原理类似）

C<sub>t</sub><sup>~</sup> = tanh(W<sub>C</sub> * [h<sub>t-1</sub> , x<sub>t</sub>] + b<sub>C</sub>)

i<sub>t</sub> = σ(W<sub>i</sub> * [h<sub>t-1</sub> , x<sub>t</sub>] + b<sub>i</sub>)

C<sub>t</sub><sup>~</sup> * i<sub>t</sub> 就是我们要新记住的信息，直接加到细胞状态中，那么我们的细胞状态更新如下：

C<sub>t</sub> = f<sub>t</sub> * C<sub>t-1</sub> + i<sub>t</sub> * C<sub>t</sub><sup>~</sup> 

- **输出门**

通过一个tanh层把细胞状态转换为输出，再通过一个类似前面遗忘门、输入门的Sigmoid层决定哪部分信息输出，输出为h<sub>t</sub>，用于下一个时间步，也可以对其进行其他处理。

&nbsp;

LSTM还有很多变体（比如GRU），但是把最原始的理解好了，后面各种变体也就很好理解了，在这里不一一介绍了。
